{"cells": [{"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import argparse\n", "import os\n", "import time\n", "import pickle\n", "import sys\n", "#numpy\n", "import numpy as np\n", "#jax\n", "import jax\n", "from jax import lax\n", "import jax.numpy as jnp\n", "import jax.random \n", "#numpyro\n", "import numpyro\n", "import numpyro.distributions as dist\n", "from numpyro.infer import MCMC, NUTS\n", "#sklearn\n", "from sklearn.preprocessing import StandardScaler\n", "#generate/ load data\n", "import get_data\n", "#visualization module\n", "import visualization\n", "#logging\n", "import logging\n", "from loader_qmap_pd import load_qmap_pd as qmap_pd"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from utils import get_infparams, get_robustK"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Set up logging"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["logging.basicConfig(\n", "    level=logging.INFO,\n", "    format='%(asctime)s - %(levelname)s - %(message)s',\n", ")\n", "logging.info('Starting run_analysis.py')"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def models(X_list, hypers, args):\n", "    #logging.debug(f\"Running models with X shape: {X.shape}, hypers:{hypers}, args: {args}\")\n", "    \n", "    logging.debug(f\"Running models with M={args.num_sources}, N={X_list[0].shape[0]}, Dm={list(hypers['Dm'])}\")\n", "    N, M = X_list[0].shape[0], args.num_sources\n", "    Dm_np = np.array(hypers['Dm'], dtype=int)   # <- static (Python / NumPy)\n", "    Dm = jnp.array(Dm_np)                       # <- JAX for computations\n", "    assert len(X_list) == M, \"Number of data sources does not match the number of provided datasets.\"\n", "    for m in range(M):\n", "        assert X_list[m].shape[0] == N, f\"Data source {m+1} has inconsistent number of samples.\"\n", "    D = int(Dm_np.sum())        # <- Python int (ok for sample_shape)\n", "    K = args.K\n", "    percW = hypers['percW']\n\n", "    # Sample sigma\n", "    sigma = numpyro.sample(\"sigma\", dist.Gamma(hypers['a_sigma'], \n", "        hypers['b_sigma']),sample_shape=(1,M)) \n", "    \n", "    if args.model == 'sparseGFA':\n", "        \n", "        # Sample Z   \n", "        Z = numpyro.sample(\"Z\",dist.Normal(0,1), sample_shape=(N,K))\n", "        # Sample tau Z\n", "        tauZ =  numpyro.sample(f'tauZ', dist.TruncatedCauchy(scale=1), sample_shape=(1,K)) \n", "        # Sample lambda Z\n", "        lmbZ =  numpyro.sample(\"lmbZ\", dist.TruncatedCauchy(scale=1), sample_shape=(N,K))\n", "        if args.reghsZ:   \n", "            # Sample cZ    \n", "            cZtmp = numpyro.sample(\"cZ\", dist.InverseGamma(0.5 * hypers['slab_df'], \n", "                0.5 * hypers['slab_df']), sample_shape=(1,K))\n", "            cZ = hypers['slab_scale'] * jnp.sqrt(cZtmp)\n", "            \n", "            # Get regularised Z\n", "            lmbZ_sqr = jnp.square(lmbZ)\n", "            for k in range(K):    \n", "                lmbZ_tilde = jnp.sqrt(lmbZ_sqr[:,k] * cZ[0,k] ** 2 / \\\n", "                    (cZ[0,k] ** 2 + tauZ[0,k] ** 2 * lmbZ_sqr[:,k]))\n", "                Z = Z.at[:,k].set(Z[:,k] * lmbZ_tilde * tauZ[0,k])\n", "        else:\n", "            Z = Z * lmbZ * tauZ\n", "    else:\n", "        # Sample Z\n", "        Z = numpyro.sample(\"Z\",dist.Normal(0,1), sample_shape=(N,K))\n", "    \n", "    #sample W\n", "    W = numpyro.sample(\"W\",dist.Normal(0,1), sample_shape=(D,K))\n", "    if 'sparseGFA' in args.model:\n", "        # Implement regularised horseshoe prior over W\n", "        #sample lambda W \n", "        lmbW =  numpyro.sample(\"lmbW\", dist.TruncatedCauchy(scale=1), sample_shape=(D,K))\n", "        #sample cW\n", "        cWtmp = numpyro.sample(\"cW\", dist.InverseGamma(0.5 * hypers['slab_df'], \n", "            0.5 * hypers['slab_df']), sample_shape=(M,K))\n", "        cW = hypers['slab_scale'] * jnp.sqrt(cWtmp)\n", "        pW = jnp.round((percW / 100.0) * Dm).astype(int)\n", "        pW = jnp.clip(pW, 1, Dm - 1)\n", "        d = 0\n", "        for m in range(M): \n", "            X_m = jnp.asarray(X_list[m])\n", "            scaleW = pW[m] / ((Dm[m] - pW[m]) * jnp.sqrt(N)) \n", "            #sample tau W\n", "            tauW =  numpyro.sample(f'tauW{m+1}', \n", "                dist.TruncatedCauchy(scale=scaleW * 1/jnp.sqrt(sigma[0,m]))) \n", "            width = int(Dm_np[m])    \n", "            lmbW_chunk = lax.dynamic_slice(lmbW, (d, 0), (width, K))\n", "     \n", "            lmbW_sqr = jnp.square(lmbW_chunk)\n", "            lmbW_tilde = jnp.sqrt(cW[m,:] ** 2 * lmbW_sqr / \n", "                (cW[m,:] ** 2 + tauW ** 2 * lmbW_sqr))\n", "            W_chunk = lax.dynamic_slice(W, (d, 0), (width, K))\n", "            W_chunk = W_chunk * lmbW_tilde * tauW\n", "            W = lax.dynamic_update_slice(W, W_chunk, (d, 0))\n", "            #sample X\n", "            W_chunk = lax.dynamic_slice(W, (d, 0), (width, K))\n", "            numpyro.sample(f'X{m+1}', dist.Normal(jnp.dot(Z, W_chunk.T), \n", "                1/jnp.sqrt(sigma[0,m])), obs=X_m)\n", "            d += width\n", "    elif args.model == 'GFA':\n", "        # Implement ARD prior over W\n", "        alpha = numpyro.sample(\"alpha\", dist.Gamma(1e-3, 1e-3), sample_shape=(M,K))\n", "        d = 0\n", "        for m in range(M): \n", "            X_m = jnp.asarray(X_list[m])\n", "            \n", "            width = int(Dm_np[m])    \n", "            \n", "            W_chunk = lax.dynamic_slice(W, (d, 0), (width, K))\n", "            W_chunk = W_chunk * (1/jnp.sqrt(alpha[m,:]))\n", "            W = lax.dynamic_update_slice(W, W_chunk, (d, 0))\n", "            #sample X\n", "            W_chunk = lax.dynamic_slice(W, (d, 0), (width, K))\n", "            numpyro.sample(f'X{m+1}', dist.Normal(jnp.dot(Z, W_chunk.T), \n", "                1/jnp.sqrt(sigma[0,m])), obs=X_m)\n", "            d += width"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def run_inference(model, args, rng_key, X_list, hypers):\n", "    \n", "    # Run inference using Hamiltonian Monte Carlo\n", "    kernel = NUTS(model, target_accept_prob=0.9, max_tree_depth=12)\n", "    mcmc = MCMC(kernel, num_warmup=args.num_warmup, num_samples=args.num_samples, \n", "        num_chains=args.num_chains)\n", "    mcmc.run(rng_key, X_list, hypers, args, extra_fields=('potential_energy',))    \n", "    #mcmc.print_summary() \n", "    return mcmc"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def main(args):                           \n", "    #Make directory to save results\n", "    if 'synthetic' in args.dataset: \n", "        flag = f'K{args.K}_{args.num_chains}chs_pW{args.percW}_s{args.num_samples}_addNoise{args.noise}'\n", "    else:\n", "        flag = f'K{args.K}_{args.num_chains}chs_pW{args.percW}_s{args.num_samples}'\n", "    \n", "    if args.model == 'sparseGFA':\n", "        flag_regZ = '_reghsZ' if args.reghsZ else '_hsZ'\n", "    else:\n", "        flag_regZ = ''\n", "    \n", "    res_dir = f'../results/{args.dataset}/{args.model}_{flag}{flag_regZ}'     \n", "    if not os.path.exists(res_dir):\n", "        os.makedirs(res_dir)  \n\n", "    # set up parameters of the priors \n", "    hp_path = f'{res_dir}/hyperparameters.dictionary'\n", "    if not os.path.exists(hp_path):   \n", "        hypers = {'a_sigma': 1, 'b_sigma': 1,\n", "                'nu_local': 1, 'nu_global': 1,\n", "                'slab_scale': 2, 'slab_df': 4, \n", "                'percW': args.percW}\n", "        with open(hp_path, 'wb') as parameters:\n", "            pickle.dump(hypers, parameters)        \n", "    else:\n", "        with open(hp_path, 'rb') as parameters:\n", "            hypers = pickle.load(parameters)       \n", "    for i in range(args.num_runs):          \n", "        logging.info(f'Initialisation: {i+1}')\n", "        logging.info('----------------------------------')\n", "        \n", "        if 'synthetic' in args.dataset:  \n", "            # Generate synthetic data \n", "            data_path = f'{res_dir}/[{i+1}]Data.dictionary'\n", "            if not os.path.exists(data_path):\n", "                data = get_data.synthetic_data(hypers, args)\n", "                with open(data_path, 'wb') as parameters:\n", "                    pickle.dump(data, parameters)\n", "            else:\n", "                with open(data_path, 'rb') as parameters:\n", "                    data = pickle.load(parameters)\n", "            X = data['X'] \n", "        elif 'qmap' in args.dataset:\n", "            data = get_data.get_data(\n", "                dataset=args.dataset,\n", "                data_dir=args.data_dir,\n", "                clinical_rel=args.clinical_rel,\n", "                volumes_rel=args.volumes_rel,\n", "                imaging_as_single_view=not args.roi_views,  # <-- convert here\n", "                id_col=args.id_col,\n", "            ) \n", "            X_list = data['X_list']\n", "            view_names = data['view_names']\n", "            args.num_sources = len(X_list)\n", "            # X = np.concatenate(X_list, axis=1)          # in case of single data view\n", "            Y = None\n", "            \n", "            logging.info(f\"qMAP-PD views: {view_names} | Dm = {[x.shape[1] for x in X_list]}\")\n", "        hypers.update({'Dm': [x.shape[1] for x in X_list] if 'X_list' in locals() else data.get('Dm')})\n", "                                                      \n", "        # RUN MODEL\n", "        res_path = f'{res_dir}/[{i+1}]Model_params.dictionary'\n", "        robparams_path = f'{res_dir}/[{i+1}]Robust_params.dictionary'\n\n", "        # Run if file doesn't exist OR is tiny/corrupted\n", "        if (not os.path.exists(res_path)) or (os.path.getsize(res_path) <= 5):\n", "            try:\n", "                logging.info('Running Model...')\n", "                seed = np.random.randint(0, 50)\n", "                rng_key = jax.random.PRNGKey(seed)\n", "                start = time.time()\n", "                MCMCout = run_inference(models, args, rng_key, X_list, hypers)\n", "                mcmc_samples = MCMCout.get_samples()\n", "                # Compute sampling performance\n", "                mcmc_samples.update({'time_elapsed': (time.time() - start)/60})\n", "                pe = MCMCout.get_extra_fields()['potential_energy']\n", "                mcmc_samples.update({'exp_logdensity': jnp.mean(-pe)})\n", "                # Save model only after success\n", "                with open(res_path, 'wb') as parameters:\n", "                    pickle.dump(mcmc_samples, parameters)\n", "                logging.info('Inferred parameters saved.')\n", "            except Exception:\n", "                logging.exception(\"MCMC run failed; not saving any placeholder.\")\n", "                # ensure no tiny/corrupt file remains\n", "                if os.path.exists(res_path) and os.path.getsize(res_path) <= 5:\n", "                    try:\n", "                        os.remove(res_path)\n", "                    except OSError:\n", "                        pass\n", "                continue  # move to next initialisation\n", "            \n", "        if (not os.path.exists(robparams_path)) and (os.path.getsize(res_path) > 5):    \n", "            #Get inferred parameters within each chain\n", "            with open(res_path, 'rb') as parameters:\n", "                mcmc_samples = pickle.load(parameters)\n", "            inf_params, data_comps = get_infparams(mcmc_samples, hypers, args)\n", "            if args.num_chains > 1:\n", "                #Find robust components\n", "                thrs = {'cosineThr': 0.8, 'matchThr': 0.5}\n", "                rob_params, X_rob, success = get_robustK(thrs, args, inf_params, data_comps)\n", "                #Save robust data components\n", "                if success:\n", "                    rob_params.update({'sigma_inf': inf_params['sigma'], 'infX': X_rob})\n", "                    if 'sparseGFA' in args.model:\n", "                        rob_params.update({'tauW_inf': inf_params['tauW']}) \n", "                    with open(robparams_path, 'wb') as parameters:\n", "                        pickle.dump(rob_params, parameters)\n", "                    logging.info('Robust parameters saved')  \n", "                else:\n", "                    logging.warning('No robust components found') \n", "            else:\n", "                W = np.mean(inf_params['W'][0], axis=0)\n", "                Z = np.mean(inf_params['Z'][0], axis=0)\n", "                X_recon = np.dot(Z, W.T)\n", "                rob_params = {'W': W, 'Z': Z, 'infX': X_recon}  \n", "                with open(robparams_path, 'wb') as parameters:\n", "                    pickle.dump(rob_params, parameters)     \n\n", "    # visualization\n", "    if 'synthetic' in args.dataset:\n", "        visualization.synthetic_data(res_dir, data, args, hypers)\n", "    else:\n", "        visualization.qmap_pd(data, res_dir, args, hypers)\n", "if __name__ == \"__main__\":\n\n", "    # Define arguments to run analysis\n", "    dataset = 'qmap'\n", "    if 'qmap' in dataset:\n", "        num_samples = 5000\n", "        K = 20\n", "        num_sources = 2\n", "        num_runs = 10\n", "    else:\n", "        num_samples = 1500\n", "        K = 5\n", "        num_sources = 3\n", "        num_runs = 5\n", "    parser = argparse.ArgumentParser(description=\" Sparse GFA with reg. horseshoe priors\")\n", "    parser.add_argument(\"--model\", nargs=\"?\", default='sparseGFA', type=str, \n", "                        help='add horseshoe prior over the latent variables')\n", "    parser.add_argument(\"--num-samples\", nargs=\"?\", default=num_samples, type=int, \n", "                        help='number of MCMC samples')\n", "    parser.add_argument(\"--num-warmup\", nargs='?', default=1000, type=int, \n", "                        help='number of MCMC samples for warmup')\n", "    parser.add_argument(\"--K\", nargs='?', default=K, type=int, \n", "                        help='number of components')\n", "    parser.add_argument(\"--num-chains\", nargs='?', default=4, type=int,\n", "                        help= 'number of MCMC chains')\n", "    parser.add_argument(\"--num-sources\", nargs='?', default=num_sources, type=int, \n", "                        help='number of data sources')\n", "    parser.add_argument(\"--num-runs\", nargs='?', default=num_runs, type=int, \n", "                        help='number of runs')\n", "    parser.add_argument(\"--reghsZ\", nargs='?', default=True, type=bool)\n", "    parser.add_argument(\"--percW\", nargs='?', default=33, type=int, \n", "                        help='percentage of relevant variables in each source')\n", "    parser.add_argument(\"--dataset\", type=str, default=\"qmap_pd\",\n", "                        choices=[\"qmap_pd\", \"synthetic\"])\n", "    parser.add_argument(\"--data_dir\", type=str, default=\"qMAP-PD_data\")\n", "    parser.add_argument(\"--device\", default='cpu', type=str, \n", "                        help='use \"cpu\" or \"gpu\".')\n", "    parser.add_argument(\"--noise\", nargs='?', default=0, type=int, \n", "                        help='Add noise to synthetic data (1=yes, 0=no)')\n", "    parser.add_argument(\"--seed\", nargs='?', default=None, type=int,\n", "                        help='Random seed for reproducibility (int). If not set, a random seed is used.')\n", "    parser.add_argument(\"--clinical_rel\", type=str, default=\"data_clinical/pd_motor_gfa_data.tsv\")\n", "    parser.add_argument(\"--volumes_rel\", type=str, default=\"volume_matrices\")\n", "    parser.add_argument(\"--id_col\", type=str, default=\"sid\")\n", "    parser.add_argument(\"--roi_views\",action=\"store_true\", \n", "                        help=\"If set, keep separate ROI views (SN/Putamen/Lentiform). If not set, concatenates imaging.\"\n", "    )\n", "    args = parser.parse_args()\n", "    \n", "    # Set the seed for reproducibility\n", "    if args.seed is not None:\n", "        seed = args.seed\n", "    else:\n", "        seed = np.random.randint(0, 100000)\n", "    np.random.seed(seed)\n", "    \n", "    numpyro.set_platform(args.device)\n", "    numpyro.set_host_device_count(args.num_chains)\n", "    main(args)"]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.6.4"}}, "nbformat": 4, "nbformat_minor": 2}